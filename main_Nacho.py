from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.sql import SQLContext
from pyspark import SparkContext
from pyspark import SparkConf
from pyspark.sql.functions import mean
import pyspark.sql.dataframe


# import org.apache.spark.sql.SQLContext

input_dataset_path = "./resources/2000.csv"

# Basically what i have written here is the Linear Regression Model as it is as well as dropping the useless columns,
# what the data cleaning is needed as well as a interface for the user to interact with our application

# The application will start here as it is executed, we can create a little console menu like in the example
if __name__ == '__main__':


    spark = SparkSession.builder.appName("Linear Regression").master("local[*]").getOrCreate()
    data = sqlContext.read.csv(path=input_dataset_path, inferSchema=True, header=True)
    # Here we delete all the instances that have been cancelled or that do not have a value in DepDelay, as well as formatting it if needed
    # beautiful cleaning code here

    #Nacho: casting de un par de variables que si no no funcionaban, en el drop eliminamos las duplicadas
    data = data.withColumn("DepDelayDouble", data["DepDelay"].cast("double"))
    data = data.withColumn("ArrDelayDouble", data["ArrDelay"].cast("double"))

    # What is CRSElapsedTime and TaxiOut??????????? I dont think we need them but good to know what they are for the report

    # Forbidden variables and useless variables divided by an intro(remember to add CRSElapsedTime and TaxiOut, not added for safety)
    data.drop("ArrTime", "ActualElapsedTime", "AirTime", "TaxiIn", "Diverted", "CarrierDelay", "WeatherDelay",
              "NASDelay", "SecurityDelay", "LateAircraftDelay", "UniqueCarrier", "FlightNum", "TailNum", "Distance",
              "Origin", "Dest", "Cancelled", "CancellationCode", "DepDelay", "ArrDelay")

    # another thing to consider: do we do more than one model like the github project?
    data.printSchema()
    data.show(20, False)

    # Prepare independent variable(feature) and dependant variable using assembler
    vector_assembler = VectorAssembler(inputCols=['DepDelayDouble'], outputCol='features')  # 2Dimensional array
    #Nacho: setHandlerInvalid para eliminar los nulls de DepDelayDouble
    input_dataset_va_df = vector_assembler.setHandleInvalid("skip").transform(data)
    input_dataset_va_fl_df = input_dataset_va_df.select(['features', 'ArrDelayDouble'])
    input_dataset_va_fl_df.show(10, False)

    # Splitting training and testing dataset 70% for training and 30% for testing
    train_test_dataset = input_dataset_va_fl_df.randomSplit([0.7, 0.3], seed=10)  # seed guarantees randomness
    print(type(train_test_dataset))
    train_dataset_df = train_test_dataset[0]
    #Nacho: Drop para eliminar los nulls
    train_dataset_df = train_dataset_df.na.drop()
    print(train_dataset_df)
    test_dataset_df = train_test_dataset[1]
    #Nacho: Drop para eliminar los nulls
    test_dataset_df = test_dataset_df.na.drop()

    # Training the model
    # I need to add in the comments what does every parameter of the LinearRegression function do
    linear_regression_model = LinearRegression(featuresCol='features',
                                               labelCol='ArrDelayDouble',
                                               maxIter=100,
                                               regParam=0.3,
                                               elasticNetParam=0.8)
    # Building the model

    linear_regression_model = linear_regression_model.fit(train_dataset_df)
    # Now we do the testing part
    predictions_df = linear_regression_model.transform(test_dataset_df)
    # The column predictions is the expected result and it is generated by the past functions
    predictions_df.select("prediction", "ArrDelayDouble", "features").show(5, False)

    # And finally, the evaluation of the model
    model_training_summary = linear_regression_model.summary
    print("RMSE %f" % model_training_summary.rootMeanSquaredError)
    print("r2: %f" % model_training_summary.r2)

    spark.stop()
    print("Demo Program Completed")
