Index: main_maria.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from pyspark.sql import SparkSession\r\nfrom pyspark.ml.feature import VectorAssembler\r\nfrom pyspark.ml.regression import LinearRegression\r\nfrom pyspark.sql import SQLContext\r\nfrom pyspark import SparkContext\r\nfrom pyspark import SparkConf\r\nfrom pyspark.sql.functions import mean\r\nimport pyspark.sql.dataframe\r\n\r\n\r\n# import org.apache.spark.sql.SQLContext\r\n\r\n#input_dataset_path = \"./resources/2000.csv\"\r\ninput_dataset_path = \"C:/Users/USUARIO/Desktop/Master HMDA/1 Semester/BIG DATA/Flight Detection/resources/2000.csv\"\r\n\r\n\r\n\r\n# The application will start here as it is executed, we can create a little console menu like in the example\r\nif __name__ == '__main__':\r\n\r\n\r\n    spark = SparkSession.builder.appName(\"Linear Regression\").master(\"local[*]\").getOrCreate()\r\n    data = spark.read.csv(path=input_dataset_path, inferSchema=True, header=True)\r\n  \r\n    print('----------------PREPROCESSING--------------------------')\r\n    #Eliminating cancelled flights \r\n    data = data[data.Cancelled != 1]\r\n\r\n    # Forbidden variables and useless variables divided by an intro(remember to add CRSElapsedTime and TaxiOut, not added for safety)\r\n    data = data.drop(\"ArrTime\", \"ActualElapsedTime\", \"AirTime\", \"TaxiIn\", \"Diverted\", \"CarrierDelay\",\r\n              \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\",\r\n\r\n              \"UniqueCarrier\", \"FlightNum\", \"TailNum\", \"Distance\", \"Origin\", \"Dest\", \"Cancelled\", \"Cancellationcode\")\r\n\r\n    #Other columns to drop because of linear dependencies: DepDelay = DepTime - CRSDepTime \r\n    data = data.drop('DepTime', 'CRSDepTime')\r\n\r\n    #Eliminating rows with missing values \r\n    data = data.dropna(how = 'any')\r\n\r\n    #Changing numerical datatypes to double  --> should we generalize and do this for all columns?  \r\n    data = data.withColumn('DepDelay', data.DepDelay.cast('double'))\r\n    data = data.withColumn('CRSArrTime', data.CRSElapsedTime.cast('double'))\r\n    data = data.withColumn('ArrDelay', data.ArrDelay.cast('double'))\r\n\r\n\r\n    data.printSchema()\r\n    data.show(10, False)\r\n\r\n    print('-----------------BUILDING MODEL----------------')\r\n    # Prepare independent variable(feature) and dependant variable using assembler\r\n    vector_assembler = VectorAssembler(inputCols=['DepDelay', 'TaxiOut', 'CRSArrTime'], outputCol='features')  \r\n    #Nacho: setHandlerInvalid para eliminar los nulls de DepDelay\r\n    input_dataset_va_df = vector_assembler.setHandleInvalid(\"skip\").transform(data)\r\n    input_dataset_va_fl_df = input_dataset_va_df.select(['features', 'ArrDelay'])\r\n    input_dataset_va_fl_df.show(10, False)\r\n\r\n    # Splitting training and testing dataset 70% for training and 30% for testing\r\n    train_test_dataset = input_dataset_va_fl_df.randomSplit([0.7, 0.3], seed=10)  # seed guarantees randomness\r\n    print(type(train_test_dataset))\r\n    train_dataset_df = train_test_dataset[0]\r\n    #Nacho: Drop para eliminar los nulls\r\n    train_dataset_df = train_dataset_df.na.drop()\r\n    print(train_dataset_df)\r\n    test_dataset_df = train_test_dataset[1]\r\n    #Nacho: Drop para eliminar los nulls\r\n    test_dataset_df = test_dataset_df.na.drop()\r\n\r\n    # Training the model\r\n    # I need to add in the comments what does every parameter of the LinearRegression function do\r\n    linear_regression_model = LinearRegression(featuresCol='features',\r\n                                               labelCol='ArrDelay',\r\n                                               maxIter=100,\r\n                                               regParam=0.2,\r\n                                               elasticNetParam=0.8)\r\n    # Building the model\r\n\r\n    linear_regression_model = linear_regression_model.fit(train_dataset_df)\r\n    # Now we do the testing part\r\n    predictions_df = linear_regression_model.transform(test_dataset_df)\r\n    # The column predictions is the expected result and it is generated by the past functions\r\n    predictions_df.select(\"prediction\", \"ArrDelay\", \"features\").show(5, False)\r\n\r\n    print('------------------MODEL EVALUATION-----------------')\r\n    # And finally, the evaluation of the model\r\n    model_training_summary = linear_regression_model.summary\r\n    print(\"RMSE %f\" % model_training_summary.rootMeanSquaredError)\r\n    print(\"r2: %f\" % model_training_summary.r2)\r\n\r\n    spark.stop()\r\n    print(\"Demo Program Completed\")\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main_maria.py b/main_maria.py
--- a/main_maria.py	(revision 0f49b99e3e5efb82ecba57f5f954652cebdb6532)
+++ b/main_maria.py	(date 1671743398810)
@@ -1,17 +1,19 @@
 from pyspark.sql import SparkSession
 from pyspark.ml.feature import VectorAssembler
-from pyspark.ml.regression import LinearRegression
+from pyspark.ml.regression import LinearRegression, RandomForestRegressor
 from pyspark.sql import SQLContext
 from pyspark import SparkContext
 from pyspark import SparkConf
 from pyspark.sql.functions import mean
 import pyspark.sql.dataframe
+from pyspark.ml.tuning import  CrossValidator, ParamGridBuilder
+from pyspark.ml.evaluation import RegressionEvaluator
 
 
 # import org.apache.spark.sql.SQLContext
 
-#input_dataset_path = "./resources/2000.csv"
-input_dataset_path = "C:/Users/USUARIO/Desktop/Master HMDA/1 Semester/BIG DATA/Flight Detection/resources/2000.csv"
+input_dataset_path = "./resources/2000.csv"
+#input_dataset_path = "C:/Users/USUARIO/Desktop/Master HMDA/1 Semester/BIG DATA/Flight Detection/resources/2000.csv"
 
 
 
@@ -29,7 +31,6 @@
     # Forbidden variables and useless variables divided by an intro(remember to add CRSElapsedTime and TaxiOut, not added for safety)
     data = data.drop("ArrTime", "ActualElapsedTime", "AirTime", "TaxiIn", "Diverted", "CarrierDelay",
               "WeatherDelay", "NASDelay", "SecurityDelay", "LateAircraftDelay",
-
               "UniqueCarrier", "FlightNum", "TailNum", "Distance", "Origin", "Dest", "Cancelled", "Cancellationcode")
 
     #Other columns to drop because of linear dependencies: DepDelay = DepTime - CRSDepTime 
@@ -38,7 +39,7 @@
     #Eliminating rows with missing values 
     data = data.dropna(how = 'any')
 
-    #Changing numerical datatypes to double  --> should we generalize and do this for all columns?  
+    #Changing numerical datatypes to double  --> should we generalize and do this for all columns?
     data = data.withColumn('DepDelay', data.DepDelay.cast('double'))
     data = data.withColumn('CRSArrTime', data.CRSElapsedTime.cast('double'))
     data = data.withColumn('ArrDelay', data.ArrDelay.cast('double'))
@@ -49,7 +50,7 @@
 
     print('-----------------BUILDING MODEL----------------')
     # Prepare independent variable(feature) and dependant variable using assembler
-    vector_assembler = VectorAssembler(inputCols=['DepDelay', 'TaxiOut', 'CRSArrTime'], outputCol='features')  
+    vector_assembler = VectorAssembler(inputCols=['DepDelay', 'TaxiOut', 'CRSArrTime'], outputCol='features')
     #Nacho: setHandlerInvalid para eliminar los nulls de DepDelay
     input_dataset_va_df = vector_assembler.setHandleInvalid("skip").transform(data)
     input_dataset_va_fl_df = input_dataset_va_df.select(['features', 'ArrDelay'])
@@ -70,22 +71,49 @@
     # I need to add in the comments what does every parameter of the LinearRegression function do
     linear_regression_model = LinearRegression(featuresCol='features',
                                                labelCol='ArrDelay',
-                                               maxIter=100,
-                                               regParam=0.2,
-                                               elasticNetParam=0.8)
+     #                                          maxIter=100,
+      #                                         regParam=0.2,
+       #                                        elasticNetParam=0.8
+                                               )
     # Building the model
 
-    linear_regression_model = linear_regression_model.fit(train_dataset_df)
+    #CrossValidator
+#    RegEvaluator = RegressionEvaluator(predictionCol="ArrDelay", labelCol="features", metricName="rmse")
+
+ #   grid = (ParamGridBuilder()
+  #                   .addGrid(linear_regression_model.regParam, [0.01, 0.1, 0.2])
+   #                  .addGrid(linear_regression_model.elasticNetParam, [0.0, 0.4, 0.8])
+    #                 .addGrid(linear_regression_model.maxIter, [1, 10, 100])
+     #              .build())
+    #cv=CrossValidator(estimator=linear_regression_model, estimatorParamMaps=grid,evaluator = RegEvaluator )
+    #input_dataset_va_fl_df=input_dataset_va_fl_df.na.drop()
+    #linear_regression_model = cv.fit(input_dataset_va_fl_df)
+    linear_regression_model=linear_regression_model.fit(train_dataset_df)
     # Now we do the testing part
+
+    Random_Forest_model = RandomForestRegressor(featuresCol='features',
+                                               labelCol='ArrDelay',numTrees=100)
+    Random_Forest_model=Random_Forest_model.fit(train_dataset_df)
     predictions_df = linear_regression_model.transform(test_dataset_df)
+    Random_Forest_model_predictions = Random_Forest_model.transform(test_dataset_df)
+
     # The column predictions is the expected result and it is generated by the past functions
     predictions_df.select("prediction", "ArrDelay", "features").show(5, False)
-
     print('------------------MODEL EVALUATION-----------------')
     # And finally, the evaluation of the model
     model_training_summary = linear_regression_model.summary
+    evaluator = RegressionEvaluator()
+    evaluator.setPredictionCol("prediction")
+    evaluator.setLabelCol("ArrDelay")
+    r2 = evaluator.evaluate(Random_Forest_model_predictions, {evaluator.metricName: "r2"})
+    rmse = evaluator.evaluate(Random_Forest_model_predictions,{evaluator.metricName: "rmse"})
+
+    print("linear regression")
     print("RMSE %f" % model_training_summary.rootMeanSquaredError)
     print("r2: %f" % model_training_summary.r2)
+    print("random forest")
+    print("RMSE %f" % rmse)
+    print("r2: %f" % r2)
 
     spark.stop()
     print("Demo Program Completed")
Index: main_nacho.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main_nacho.py b/main_nacho.py
new file mode 100644
--- /dev/null	(date 1671759953804)
+++ b/main_nacho.py	(date 1671759953804)
@@ -0,0 +1,151 @@
+from pyspark.sql import SparkSession
+from pyspark.ml import Pipeline
+from pyspark.ml.feature import VectorAssembler,VectorIndexer
+from pyspark.ml.regression import LinearRegression, RandomForestRegressor,IsotonicRegression
+from pyspark.sql import SQLContext
+from pyspark import SparkContext
+from pyspark import SparkConf
+from pyspark.sql.functions import mean
+import pyspark.sql.dataframe
+from pyspark.ml.feature import VectorIndexer
+from pyspark.ml.evaluation import RegressionEvaluator
+
+
+
+# import org.apache.spark.sql.SQLContext
+
+input_dataset_path = "./resources/2000.csv"
+#input_dataset_path = "C:/Users/USUARIO/Desktop/Master HMDA/1 Semester/BIG DATA/Flight Detection/resources/2000.csv"
+
+# The application will start here as it is executed, we can create a little console menu like in the example
+if __name__ == '__main__':
+    spark = SparkSession.builder.appName("Linear Regression").master("local[*]").getOrCreate()
+    data = spark.read.csv(path=input_dataset_path, inferSchema=True, header=True)
+
+    print('----------------PREPROCESSING--------------------------')
+    # Eliminating cancelled flights
+    data = data[data.Cancelled != 1]
+
+    # Forbidden variables and useless variables divided by an intro (remember to add CRSElapsedTime and TaxiOut, not added for safety)
+    data = data.drop("ArrTime", "ActualElapsedTime", "AirTime", "TaxiIn", "Diverted", "CarrierDelay",
+                     "WeatherDelay", "NASDelay", "SecurityDelay", "LateAircraftDelay",
+
+                     "UniqueCarrier", "FlightNum", "TailNum", "Origin", "Dest", "Cancelled",
+                     "Cancellationcode", "Year", "Month", 'DayOfWeek',"DayofMonth", "CRSElapsedTime")
+
+    # Other columns to drop because of linear dependencies: DepDelay = DepTime - CRSDepTime
+    data = data.drop('DepTime', 'CRSDepTime')
+
+    # Changing numerical datatypes to double  --> should we generalize and do this for all columns?
+    data = data.withColumn('DepDelay', data.DepDelay.cast('double'))
+    data = data.withColumn('CRSArrTime', data.CRSArrTime.cast('double'))
+    data = data.withColumn('ArrDelay', data.ArrDelay.cast('double'))
+    data = data.withColumn('TaxiOut', data.TaxiOut.cast('double'))
+    data = data.withColumn('Distance', data.TaxiOut.cast('double'))
+
+    # Droping the rows with missing values in the target variable
+    data = data.na.drop(subset=["ArrDelay"])
+
+    # Filling in missing values with 0 in the selected feature columns
+    data = data.na.fill(value=0)
+    data.show(5, False)
+
+    data.printSchema()
+    data.show(10, False)
+
+    print('-----------------BUILDING MODEL----------------')
+    # Prepare independent variable(feature) and dependant variable using assembler
+    inputCols = ['DepDelay', 'TaxiOut', 'CRSArrTime', 'Distance']
+
+    vector_assembler = VectorAssembler(inputCols=inputCols, outputCol='features')
+    # Nacho: setHandlerInvalid para eliminar los nulls de DepDelay
+    input_dataset_va_df = vector_assembler.setHandleInvalid("skip").transform(data)
+    input_dataset_va_fl_df = input_dataset_va_df.select(['features', 'ArrDelay'])
+    input_dataset_va_fl_df.show(10, False)
+
+    # Splitting training and testing dataset 70% for training and 30% for testing
+    train_test_dataset = input_dataset_va_fl_df.randomSplit([0.7, 0.3], seed=10)  # seed guarantees randomness
+    print(type(train_test_dataset))
+    train_dataset_df = train_test_dataset[0]
+    test_dataset_df = train_test_dataset[1]
+    #fitting data for decision tree
+    featureIndexer = VectorIndexer(inputCol="features",
+                                   outputCol="indexedFeatures",
+                                   maxCategories=300).fit(input_dataset_va_fl_df)
+
+
+
+
+    # Training the model
+    linear_regression_model = LinearRegression(featuresCol='features',
+                                               labelCol='ArrDelay',
+                                               maxIter=100,
+                                                regParam=0.2,
+                                               elasticNetParam=0.8)
+
+    dt = RandomForestRegressor(featuresCol="indexedFeatures", labelCol='ArrDelay')
+
+    Random_Forest_model = RandomForestRegressor(featuresCol='features',
+                                                labelCol='ArrDelay')
+    Isotonic_regression_model = IsotonicRegression(featuresCol='features',
+                                                labelCol='ArrDelay')
+
+    RegEvaluator = RegressionEvaluator(predictionCol="ArrDelay", labelCol="features", metricName="r2")
+
+
+    # Building the model
+    linear_regression_model = linear_regression_model.fit(train_dataset_df)
+
+    pipeline = Pipeline(stages=[featureIndexer, dt])
+    model = pipeline.fit(train_dataset_df)
+
+    Random_Forest_model=Random_Forest_model.fit(train_dataset_df)
+
+    Isotonic_regression_model = Isotonic_regression_model.fit(train_dataset_df)
+
+    # Testing the model
+    predictions_df = linear_regression_model.transform(test_dataset_df)
+    Random_Forest_model_predictions = Random_Forest_model.transform(test_dataset_df)
+
+    predictions_rf = model.transform(test_dataset_df)
+    predictions_ir = Isotonic_regression_model.transform(test_dataset_df)
+
+    # The column predictions is the expected result and it is generated by the past functions
+    predictions_df.select("prediction", "ArrDelay", "features").show(5, False)
+    Random_Forest_model_predictions.select("prediction", "ArrDelay", "features").show(5, False)
+
+
+    print('------------------MODEL EVALUATION-----------------')
+    # And finally, the evaluation of the model
+    model_training_summary = linear_regression_model.summary
+
+    evaluator = RegressionEvaluator()
+    evaluator.setPredictionCol("prediction")
+    evaluator.setLabelCol("ArrDelay")
+    r2_dt = evaluator.evaluate(predictions_rf, {evaluator.metricName: "r2"})
+    rmse_dt = evaluator.evaluate(predictions_rf, {evaluator.metricName: "rmse"})
+
+    r2 = evaluator.evaluate(Random_Forest_model_predictions, {evaluator.metricName: "r2"})
+    rmse = evaluator.evaluate(Random_Forest_model_predictions, {evaluator.metricName: "rmse"})
+
+    r2_ir = evaluator.evaluate(predictions_ir, {evaluator.metricName: "r2"})
+    rmse_ir = evaluator.evaluate(predictions_ir, {evaluator.metricName: "rmse"})
+
+    print("linear regression")
+    print("RMSE %f" % model_training_summary.rootMeanSquaredError)
+    print("r2: %f" % model_training_summary.r2)
+
+    print("decision tree")
+    print("RMSE %f" % rmse_dt)
+    print("r2: %f" % r2_dt)
+
+    print("random forest")
+    print("RMSE %f" % rmse)
+    print("r2: %f" % r2)
+
+    print(" Isotonic_regression")
+    print("RMSE %f" % rmse_ir)
+    print("r2: %f" % r2_ir)
+
+    spark.stop()
+    print("Demo Program Completed")
\ No newline at end of file
