Index: main_maria.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from pyspark.sql import SparkSession\r\nfrom pyspark.ml.feature import VectorAssembler\r\nfrom pyspark.ml.regression import LinearRegression\r\nfrom pyspark.sql import SQLContext\r\nfrom pyspark import SparkContext\r\nfrom pyspark import SparkConf\r\nfrom pyspark.sql.functions import mean\r\nimport pyspark.sql.dataframe\r\n\r\n\r\n# import org.apache.spark.sql.SQLContext\r\n\r\n#input_dataset_path = \"./resources/2000.csv\"\r\ninput_dataset_path = \"C:/Users/USUARIO/Desktop/Master HMDA/1 Semester/BIG DATA/Flight Detection/resources/2000.csv\"\r\n\r\n# Basically what i have written here is the Linear Regression Model as it is as well as dropping the useless columns,\r\n# what the data cleaning is needed as well as a interface for the user to interact with our application\r\n\r\n# The application will start here as it is executed, we can create a little console menu like in the example\r\nif __name__ == '__main__':\r\n\r\n\r\n    spark = SparkSession.builder.appName(\"Linear Regression\").master(\"local[*]\").getOrCreate()\r\n    data = spark.read.csv(path=input_dataset_path, inferSchema=True, header=True)\r\n    # Here we delete all the instances that have been cancelled or that do not have a value in DepDelay, as well as formatting it if needed\r\n    # beautiful cleaning code here\r\n    print('----------------PREPROCESSING--------------------------')\r\n    #Eliminating cancelled flights \r\n    data = data[data.Cancelled != 1]\r\n\r\n    # Forbidden variables and useless variables divided by an intro(remember to add CRSElapsedTime and TaxiOut, not added for safety)\r\n    data = data.drop(\"ArrTime\", \"ActualElapsedTime\", \"AirTime\", \"TaxiIn\", \"Diverted\", \"CarrierDelay\",\r\n              \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\",\r\n\r\n              \"UniqueCarrier\", \"FlightNum\", \"TailNum\", \"Distance\", \"Origin\", \"Dest\", \"Cancelled\", \"Cancellationcode\")\r\n\r\n    #Other columns to drop because of linear dependencies: DepDelay = DepTime - CRSDepTime \r\n    data = data.drop('DepTime', 'CRSDepTime')\r\n\r\n    #Eliminating rows with missing values \r\n    data = data.dropna(how = 'any')\r\n\r\n    #Changing numerical datatypes to double  --> should we generalize and do this for all columns?  \r\n    data = data.withColumn('DepDelay', data.DepDelay.cast('double'))\r\n    data = data.withColumn('CRSElapsedTime', data.CRSElapsedTime.cast('double'))\r\n    data = data.withColumn('ArrDelay', data.ArrDelay.cast('double'))\r\n\r\n\r\n    data.printSchema()\r\n    data.show(10, False)\r\n\r\n    print('-----------------BUILDING MODEL----------------')\r\n    # Prepare independent variable(feature) and dependant variable using assembler\r\n    vector_assembler = VectorAssembler(inputCols=['DepDelay', 'TaxiOut', 'CRSArrTime'], outputCol='features')  # 2Dimensional array\r\n    #Nacho: setHandlerInvalid para eliminar los nulls de DepDelay\r\n    input_dataset_va_df = vector_assembler.setHandleInvalid(\"skip\").transform(data)\r\n    input_dataset_va_fl_df = input_dataset_va_df.select(['features', 'ArrDelay'])\r\n    input_dataset_va_fl_df.show(10, False)\r\n\r\n    # Splitting training and testing dataset 70% for training and 30% for testing\r\n    train_test_dataset = input_dataset_va_fl_df.randomSplit([0.7, 0.3], seed=10)  # seed guarantees randomness\r\n    print(type(train_test_dataset))\r\n    train_dataset_df = train_test_dataset[0]\r\n    #Nacho: Drop para eliminar los nulls\r\n    train_dataset_df = train_dataset_df.na.drop()\r\n    print(train_dataset_df)\r\n    test_dataset_df = train_test_dataset[1]\r\n    #Nacho: Drop para eliminar los nulls\r\n    test_dataset_df = test_dataset_df.na.drop()\r\n\r\n    # Training the model\r\n    # I need to add in the comments what does every parameter of the LinearRegression function do\r\n    linear_regression_model = LinearRegression(featuresCol='features',\r\n                                               labelCol='ArrDelay',\r\n                                               maxIter=100,\r\n                                               regParam=0.3,\r\n                                               elasticNetParam=0.8)\r\n    # Building the model\r\n\r\n    linear_regression_model = linear_regression_model.fit(train_dataset_df)\r\n    # Now we do the testing part\r\n    predictions_df = linear_regression_model.transform(test_dataset_df)\r\n    # The column predictions is the expected result and it is generated by the past functions\r\n    predictions_df.select(\"prediction\", \"ArrDelay\", \"features\").show(5, False)\r\n\r\n    print('------------------MODEL EVALUATION-----------------')\r\n    # And finally, the evaluation of the model\r\n    model_training_summary = linear_regression_model.summary\r\n    print(\"RMSE %f\" % model_training_summary.rootMeanSquaredError)\r\n    print(\"r2: %f\" % model_training_summary.r2)\r\n\r\n    spark.stop()\r\n    print(\"Demo Program Completed\")
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main_maria.py b/main_maria.py
--- a/main_maria.py	(revision 3b3400b2cba3b122170cc7034c28fc4d83c6be1b)
+++ b/main_maria.py	(date 1670949652722)
@@ -10,8 +10,8 @@
 
 # import org.apache.spark.sql.SQLContext
 
-#input_dataset_path = "./resources/2000.csv"
-input_dataset_path = "C:/Users/USUARIO/Desktop/Master HMDA/1 Semester/BIG DATA/Flight Detection/resources/2000.csv"
+input_dataset_path = "./resources/2000.csv"
+#input_dataset_path = "C:/Users/USUARIO/Desktop/Master HMDA/1 Semester/BIG DATA/Flight Detection/resources/2000.csv"
 
 # Basically what i have written here is the Linear Regression Model as it is as well as dropping the useless columns,
 # what the data cleaning is needed as well as a interface for the user to interact with our application
@@ -31,7 +31,6 @@
     # Forbidden variables and useless variables divided by an intro(remember to add CRSElapsedTime and TaxiOut, not added for safety)
     data = data.drop("ArrTime", "ActualElapsedTime", "AirTime", "TaxiIn", "Diverted", "CarrierDelay",
               "WeatherDelay", "NASDelay", "SecurityDelay", "LateAircraftDelay",
-
               "UniqueCarrier", "FlightNum", "TailNum", "Distance", "Origin", "Dest", "Cancelled", "Cancellationcode")
 
     #Other columns to drop because of linear dependencies: DepDelay = DepTime - CRSDepTime 
Index: main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from pyspark.sql import SparkSession\r\nfrom pyspark.ml.feature import VectorAssembler\r\nfrom pyspark.ml.regression import LinearRegression\r\n\r\ninput_dataset_path = \"./resources/2000.csv\"\r\n\r\n# Basically what i have written here is the Linear Regression Model as it is as well as dropping the useless columns,\r\n# what the data cleaning is needed as well as a interface for the user to interact with our application\r\n\r\n# The application will start here as it is executed, we can create a little console menu like in the example\r\nif __name__ == '__main__':\r\n    spark = SparkSession.builder.appName(\"Linear Regression\").master(\"local[*]\").getOrCreate()\r\n    data = spark.read.csv(path=input_dataset_path, inferSchema=True, header=True)\r\n    # Here we delete all the instances that have been cancelled or that do not have a value in DepDelay, as well as formatting it if needed\r\n    # beautiful cleaning code here\r\n    # What is CRSElapsedTime and TaxiOut??????????? I dont think we need them but good to know what they are for the report\r\n\r\n    # Forbidden variables and useless variables divided by an intro(remember to add CRSElapsedTime and TaxiOut, not added for safety)\r\n    data.drop(\"ArrTime\", \"ActualElapsedTime\", \"AirTime\", \"TaxiIn\", \"Diverted\", \"CarrierDelay\",\r\n              \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\",\r\n\r\n              \"UniqueCarrier\", \"FlightNum\", \"TailNum\", \"Distance\", \"Origin\", \"Dest\", \"Cancelled\", \"Cancellationcode\")\r\n\r\n    # another thing to consider: do we do more than one model like the github project?\r\n\r\n    data.printSchema()\r\n    data.show(10, False)\r\n\r\n    # Prepare independent variable(feature) and dependant variable using assembler\r\n    vector_assembler = VectorAssembler(inputCols=['DepDelay'], outputCol='features')  # 2Dimensional array\r\n    input_dataset_va_df = vector_assembler.transform(data)\r\n    input_dataset_va_fl_df = input_dataset_va_df.select(['features', 'ArrDelay'])\r\n    input_dataset_va_fl_df.show(5, False)\r\n\r\n    # Splitting training and testing dataset 70% for training and 30% for testing\r\n    train_test_dataset = input_dataset_va_fl_df.randomSplit([0.7, 0.3], seed=10)  # seed guarantees randomness\r\n    print(type(train_test_dataset))\r\n    train_dataset_df = train_test_dataset[0]\r\n    test_dataset_df = train_test_dataset[1]\r\n\r\n    # Training the model\r\n    # I need to add in the comments what does every parameter of the LinearRegression function do\r\n    linear_regression_model = LinearRegression(featuresCol='features',\r\n                                               labelCol='ArrDelay',\r\n                                               maxIter=100,\r\n                                               regParam=0.3,\r\n                                               elasticNetParam=0.8)\r\n    # Building the model\r\n    linear_regression_model = linear_regression_model.fit(train_dataset_df)\r\n\r\n    # Now we do the testing part\r\n    predictions_df = linear_regression_model.transform(test_dataset_df)\r\n    # The column predictions is the expected result and it is generated by the past functions\r\n    predictions_df.select(\"prediction\", \"ArrDelay\", \"features\").show(5, False)\r\n\r\n    # And finally, the evaluation of the model\r\n    model_training_summary = linear_regression_model.summary\r\n    print(\"RMSE %f\" % model_training_summary.rootMeanSquaredError)\r\n    print(\"r2: %f\" % model_training_summary.r2)\r\n\r\n    spark.stop()\r\n    print(\"Demo Program Completed\")\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main.py b/main.py
--- a/main.py	(revision 3b3400b2cba3b122170cc7034c28fc4d83c6be1b)
+++ b/main.py	(date 1670949194223)
@@ -18,7 +18,6 @@
     # Forbidden variables and useless variables divided by an intro(remember to add CRSElapsedTime and TaxiOut, not added for safety)
     data.drop("ArrTime", "ActualElapsedTime", "AirTime", "TaxiIn", "Diverted", "CarrierDelay",
               "WeatherDelay", "NASDelay", "SecurityDelay", "LateAircraftDelay",
-
               "UniqueCarrier", "FlightNum", "TailNum", "Distance", "Origin", "Dest", "Cancelled", "Cancellationcode")
 
     # another thing to consider: do we do more than one model like the github project?
Index: main_Nacho.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from pyspark.sql import SparkSession\r\nfrom pyspark.ml.feature import VectorAssembler\r\nfrom pyspark.ml.regression import LinearRegression\r\nfrom pyspark.sql import SQLContext\r\nfrom pyspark import SparkContext\r\nfrom pyspark import SparkConf\r\nfrom pyspark.sql.functions import mean\r\nimport pyspark.sql.dataframe\r\n\r\n\r\n# import org.apache.spark.sql.SQLContext\r\n\r\ninput_dataset_path = \"./resources/2000.csv\"\r\n\r\n# Basically what i have written here is the Linear Regression Model as it is as well as dropping the useless columns,\r\n# what the data cleaning is needed as well as a interface for the user to interact with our application\r\n\r\n# The application will start here as it is executed, we can create a little console menu like in the example\r\nif __name__ == '__main__':\r\n\r\n\r\n    spark = SparkSession.builder.appName(\"Linear Regression\").master(\"local[*]\").getOrCreate()\r\n    data = sqlContext.read.csv(path=input_dataset_path, inferSchema=True, header=True)\r\n    # Here we delete all the instances that have been cancelled or that do not have a value in DepDelay, as well as formatting it if needed\r\n    # beautiful cleaning code here\r\n\r\n    #Nacho: casting de un par de variables que si no no funcionaban, en el drop eliminamos las duplicadas\r\n    data = data.withColumn(\"DepDelayDouble\", data[\"DepDelay\"].cast(\"double\"))\r\n    data = data.withColumn(\"ArrDelayDouble\", data[\"ArrDelay\"].cast(\"double\"))\r\n\r\n    # What is CRSElapsedTime and TaxiOut??????????? I dont think we need them but good to know what they are for the report\r\n\r\n    # Forbidden variables and useless variables divided by an intro(remember to add CRSElapsedTime and TaxiOut, not added for safety)\r\n    data.drop(\"ArrTime\", \"ActualElapsedTime\", \"AirTime\", \"TaxiIn\", \"Diverted\", \"CarrierDelay\", \"WeatherDelay\",\r\n              \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\", \"UniqueCarrier\", \"FlightNum\", \"TailNum\", \"Distance\",\r\n              \"Origin\", \"Dest\", \"Cancelled\", \"CancellationCode\", \"DepDelay\", \"ArrDelay\")\r\n\r\n    # another thing to consider: do we do more than one model like the github project?\r\n    data.printSchema()\r\n    data.show(20, False)\r\n\r\n    # Prepare independent variable(feature) and dependant variable using assembler\r\n    vector_assembler = VectorAssembler(inputCols=['DepDelayDouble'], outputCol='features')  # 2Dimensional array\r\n    #Nacho: setHandlerInvalid para eliminar los nulls de DepDelayDouble\r\n    input_dataset_va_df = vector_assembler.setHandleInvalid(\"skip\").transform(data)\r\n    input_dataset_va_fl_df = input_dataset_va_df.select(['features', 'ArrDelayDouble'])\r\n    input_dataset_va_fl_df.show(10, False)\r\n\r\n    # Splitting training and testing dataset 70% for training and 30% for testing\r\n    train_test_dataset = input_dataset_va_fl_df.randomSplit([0.7, 0.3], seed=10)  # seed guarantees randomness\r\n    print(type(train_test_dataset))\r\n    train_dataset_df = train_test_dataset[0]\r\n    #Nacho: Drop para eliminar los nulls\r\n    train_dataset_df = train_dataset_df.na.drop()\r\n    print(train_dataset_df)\r\n    test_dataset_df = train_test_dataset[1]\r\n    #Nacho: Drop para eliminar los nulls\r\n    test_dataset_df = test_dataset_df.na.drop()\r\n\r\n    # Training the model\r\n    # I need to add in the comments what does every parameter of the LinearRegression function do\r\n    linear_regression_model = LinearRegression(featuresCol='features',\r\n                                               labelCol='ArrDelayDouble',\r\n                                               maxIter=100,\r\n                                               regParam=0.3,\r\n                                               elasticNetParam=0.8)\r\n    # Building the model\r\n\r\n    linear_regression_model = linear_regression_model.fit(train_dataset_df)\r\n    # Now we do the testing part\r\n    predictions_df = linear_regression_model.transform(test_dataset_df)\r\n    # The column predictions is the expected result and it is generated by the past functions\r\n    predictions_df.select(\"prediction\", \"ArrDelayDouble\", \"features\").show(5, False)\r\n\r\n    # And finally, the evaluation of the model\r\n    model_training_summary = linear_regression_model.summary\r\n    print(\"RMSE %f\" % model_training_summary.rootMeanSquaredError)\r\n    print(\"r2: %f\" % model_training_summary.r2)\r\n\r\n    spark.stop()\r\n    print(\"Demo Program Completed\")\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main_Nacho.py b/main_Nacho.py
--- a/main_Nacho.py	(revision 3b3400b2cba3b122170cc7034c28fc4d83c6be1b)
+++ b/main_Nacho.py	(date 1670949318435)
@@ -20,7 +20,7 @@
 
 
     spark = SparkSession.builder.appName("Linear Regression").master("local[*]").getOrCreate()
-    data = sqlContext.read.csv(path=input_dataset_path, inferSchema=True, header=True)
+    data = spark.read.csv(path=input_dataset_path, inferSchema=True, header=True)
     # Here we delete all the instances that have been cancelled or that do not have a value in DepDelay, as well as formatting it if needed
     # beautiful cleaning code here
 
